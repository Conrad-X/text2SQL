import os
from datetime import datetime
from typing import List, Optional

import concurrent.futures
from loguru import logger
import pandas as pd
from snowflake.connector import DictCursor
from post_process import (
    proto_to_yaml,
    validate_context_length
)
from datatypes import(
    Column,
    Table
)
import semantic_model_pb2
from snowflake_connector import (
    get_table_comment
)


TIME_MEASURE_DATATYPES = [
    "DATE",
    "DATETIME",
    "TIMESTAMP_LTZ",
    "TIMESTAMP_NTZ",
    "TIMESTAMP_TZ",
    "TIMESTAMP",
    "TIME",
]

DIMENSION_DATATYPES = [
    "VARCHAR",
    "CHAR",
    "CHARACTER",
    "NCHAR",
    "STRING",
    "TEXT",
    "NVARCHAR",
    "NVARCHAR2",
    "CHAR VARYING",
    "NCHAR VARYING",
    "BINARY",
    "VARBINARY",
]

MEASURE_DATATYPES = [
    "NUMBER",
    "DECIMAL",
    "DEC",
    "NUMERIC",
    "INT",
    "INTEGER",
    "BIGINT",
    "SMALLINT",
    "TINYINT",
    "BYTEINT",
    "FLOAT",
    "FLOAT4",
    "FLOAT8",
    "DOUBLE",
    "DOUBLE PRECISION",
    "REAL",
]
OBJECT_DATATYPES = ["VARIANT", "ARRAY", "OBJECT", "GEOGRAPHY"]


_QUERY_TAG = "SEMANTIC_MODEL_GENERATOR"

_PLACEHOLDER_COMMENT = "  "
_FILL_OUT_TOKEN = " # <FILL-OUT>"
AUTOGEN_TOKEN = "__"
# TODO add _AUTO_GEN_TOKEN to the end of the auto generated descriptions.
_AUTOGEN_COMMENT_TOKEN = (
    " # <AUTO-GENERATED DESCRIPTION, PLEASE MODIFY AND REMOVE THE __ AT THE END>"
)
_DEFAULT_N_SAMPLE_VALUES_PER_COL = 3
_AUTOGEN_COMMENT_WARNING = f"# NOTE: This file was auto-generated by the semantic model generator. Please fill out placeholders marked with {_FILL_OUT_TOKEN} (or remove if not relevant) and verify autogenerated comments.\n"
_COMMENT_COL = "COMMENT"
_COLUMN_NAME_COL = "COLUMN_NAME"
_DATATYPE_COL = "DATA_TYPE"
_TABLE_SCHEMA_COL = "TABLE_SCHEMA"
_TABLE_NAME_COL = "TABLE_NAME"
# Below are the renamed column names when we fetch into dataframe, to differentiate between table/column comments
_COLUMN_COMMENT_ALIAS = "COLUMN_COMMENT"
_TABLE_COMMENT_COL = "TABLE_COMMENT"

def get_column_representation(
    conn,
    schema_name: str,
    table_name: str,
    column_row: pd.Series,
    column_index: int,
    ndv: int,
):
    column_name = column_row[_COLUMN_NAME_COL]
    column_datatype = column_row[_DATATYPE_COL]
    column_values = None
    if ndv > 0:
        # Pull sample values.
        try:
            cursor = conn.cursor(DictCursor)
            assert cursor is not None, "Cursor is unexpectedly None"
            cursor_execute = cursor.execute(
                f'select distinct "{column_name}" from {schema_name}.{table_name} limit {ndv}'
            )
            assert cursor_execute is not None, "cursor_execute should not be none "
            res = cursor_execute.fetchall()
            # Cast all values to string to ensure the list is json serializable.
            # A better solution would be to identify the possible types that are not
            # json serializable (e.g. datetime objects) and apply the appropriate casting
            # in just those cases.
            if len(res) > 0:
                if isinstance(res[0], dict):
                    col_key = [k for k in res[0].keys()][0]
                    column_values = [str(r[col_key]) for r in res]
                else:
                    raise ValueError(
                        f"Expected the first item of res to be a dict. Instead passed {res}"
                    )
        except Exception as e:
            logger.error(f"unable to get values: {e}")

    # column_comment = _get_column_comment(conn, column_row, column_values)
    column_comment="No Comment"

    column = Column(
        id_=column_index,
        column_name=column_name,
        comment=column_comment,
        column_type=column_datatype,
        values=column_values,
    )
    return column

def get_table_representation(
    conn,
    schema_name: str,
    table_name: str,
    table_index: int,
    ndv_per_column: int,
    columns_df: pd.DataFrame,
    max_workers: int,
) -> Table:
    table_comment = get_table_comment(conn, schema_name, table_name, columns_df)

    def _get_col(col_index: int, column_row: pd.Series) -> Column:
        return get_column_representation(
            conn=conn,
            schema_name=schema_name,
            table_name=table_name,
            column_row=column_row,
            column_index=col_index,
            ndv=ndv_per_column,
        )

    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_col_index = {
            executor.submit(_get_col, col_index, column_row): col_index
            for col_index, (_, column_row) in enumerate(columns_df.iterrows())
        }
        index_and_column = []
        for future in concurrent.futures.as_completed(future_to_col_index):
            col_index = future_to_col_index[future]
            column = future.result()
            index_and_column.append((col_index, column))
        columns = [c for _, c in sorted(index_and_column, key=lambda x: x[0])]

    return Table(
        id_=table_index,
        name=table_name,
        comment=table_comment,
        columns=columns,
    )

def fetch_valid_tables_and_views(
    conn, db_name: str
) -> pd.DataFrame:
    def _get_df(query: str) -> pd.DataFrame:
        cursor = conn.cursor().execute(query)
        assert cursor is not None, "cursor should not be none here."

        df = pd.DataFrame(
            cursor.fetchall(), columns=[c.name for c in cursor.description]
        )
        return df[["name", "schema_name", "comment"]].rename(
            columns=dict(
                name=_TABLE_NAME_COL,
                schema_name=_TABLE_SCHEMA_COL,
                comment=_TABLE_COMMENT_COL,
            )
        )

    tables = _get_df(f"show tables in database {db_name}")
    views = _get_df(f"show views in database {db_name}")
    return pd.concat([tables, views], axis=0)


def get_valid_schemas_tables_columns_df(
    conn,
    db_name: str,
    table_schema: Optional[str] = None,
    table_names: Optional[List[str]] = None,
) -> pd.DataFrame:
    if table_names and not table_schema:
        logger.warning(
            "Provided table_name without table_schema, cannot filter to fetch the specific table"
        )
    where_clause = ""
    if table_schema:
        where_clause += f" where t.table_schema ilike '{table_schema}' "
        if table_names:
            table_names_str = ", ".join([f"'{t.lower()}'" for t in table_names])
            where_clause += f"AND LOWER(t.table_name) in ({table_names_str}) "
    query = f"""select t.{_TABLE_SCHEMA_COL}, t.{_TABLE_NAME_COL}, c.{_COLUMN_NAME_COL}, c.{_DATATYPE_COL}, c.{_COMMENT_COL} as {_COLUMN_COMMENT_ALIAS} from {db_name}.information_schema.tables as t join {db_name}.information_schema.columns as c on t.table_schema = c.table_schema and t.table_name = c.table_name{where_clause} order by 1, 2, c.ordinal_position"""
    cursor_execute = conn.cursor().execute(query)
    assert cursor_execute, "cursor_execute should not be None here"
    schemas_tables_columns_df = cursor_execute.fetch_pandas_all()

    valid_tables_and_views_df = fetch_valid_tables_and_views(
        conn=conn, db_name=db_name
    )

    valid_schemas_tables_columns_df = valid_tables_and_views_df.merge(
        schemas_tables_columns_df, how="inner", on=(_TABLE_SCHEMA_COL, _TABLE_NAME_COL)
    )
    return valid_schemas_tables_columns_df

def _get_placeholder_filter() -> List[semantic_model_pb2.NamedFilter]:
    return [
        semantic_model_pb2.NamedFilter(
            name=_PLACEHOLDER_COMMENT,
            synonyms=[_PLACEHOLDER_COMMENT],
            description=_PLACEHOLDER_COMMENT,
            expr=_PLACEHOLDER_COMMENT,
        )
    ]


def _get_placeholder_joins() -> List[semantic_model_pb2.Relationship]:
    return [
        semantic_model_pb2.Relationship(
            name=_PLACEHOLDER_COMMENT,
            left_table=_PLACEHOLDER_COMMENT,
            right_table=_PLACEHOLDER_COMMENT,
            join_type=semantic_model_pb2.JoinType.inner,
            relationship_columns=[
                semantic_model_pb2.RelationKey(
                    left_column=_PLACEHOLDER_COMMENT,
                    right_column=_PLACEHOLDER_COMMENT,
                )
            ],
            relationship_type=semantic_model_pb2.RelationshipType.many_to_one,
        )
    ]


def _raw_table_to_semantic_context_table(
    database: str, schema: str, raw_table
) :
    """
    Converts a raw table representation to a semantic model table in protobuf format.

    Args:
        database (str): The name of the database containing the table.
        schema (str): The name of the schema containing the table.
        raw_table (data_types.Table): The raw table object to be transformed.

    Returns:
        semantic_model_pb2.Table: A protobuf representation of the semantic table.

    This function categorizes table columns into TimeDimensions, Dimensions, or Measures based on their data type,
    populates them with sample values, and sets placeholders for descriptions and filters.
    """

    # For each column, decide if it is a TimeDimension, Measure, or Dimension column.
    # For now, we decide this based on datatype.
    # Any time datatype, is TimeDimension.
    # Any varchar/text is Dimension.
    # Any numerical column is Measure.

    time_dimensions = []
    dimensions = []
    measures = []

    for col in raw_table.columns:
        if col.column_type.upper() in TIME_MEASURE_DATATYPES:
            time_dimensions.append(
                semantic_model_pb2.TimeDimension(
                    name=col.column_name,
                    expr=col.column_name,
                    data_type=col.column_type,
                    sample_values=col.values,
                    synonyms=[_PLACEHOLDER_COMMENT],
                    description=col.comment if col.comment else _PLACEHOLDER_COMMENT,
                )
            )

        elif col.column_type.upper() in DIMENSION_DATATYPES:
            dimensions.append(
                semantic_model_pb2.Dimension(
                    name=col.column_name,
                    expr=col.column_name,
                    data_type=col.column_type,
                    sample_values=col.values,
                    synonyms=[_PLACEHOLDER_COMMENT],
                    description=col.comment if col.comment else _PLACEHOLDER_COMMENT,
                )
            )

        elif col.column_type.upper() in MEASURE_DATATYPES:
            measures.append(
                semantic_model_pb2.Measure(
                    name=col.column_name,
                    expr=col.column_name,
                    data_type=col.column_type,
                    sample_values=col.values,
                    synonyms=[_PLACEHOLDER_COMMENT],
                    description=col.comment if col.comment else _PLACEHOLDER_COMMENT,
                )
            )
        elif col.column_type.upper() in OBJECT_DATATYPES:
            logger.warning(
                f"""We don't currently support {col.column_type} as an input column datatype to the Semantic Model. We are skipping column {col.column_name} for now."""
            )
            continue
        else:
            logger.warning(
                f"Column datatype does not map to a known datatype. Input was = {col.column_type}. We are going to place as a Dimension for now."
            )
            dimensions.append(
                semantic_model_pb2.Dimension(
                    name=col.column_name,
                    expr=col.column_name,
                    data_type=col.column_type,
                    sample_values=col.values,
                    synonyms=[_PLACEHOLDER_COMMENT],
                    description=col.comment if col.comment else _PLACEHOLDER_COMMENT,
                )
            )
    if len(time_dimensions) + len(dimensions) + len(measures) == 0:
        raise ValueError(
            f"No valid columns found for table {raw_table.name}. Please verify that this table contains column's datatypes not in {OBJECT_DATATYPES}."
        )

    return semantic_model_pb2.Table(
        name=raw_table.name,
        base_table=semantic_model_pb2.FullyQualifiedTable(
            database=database, schema=schema, table=raw_table.name
        ),
        # For fields we can not automatically infer, leave a comment for the user to fill out.
        description=raw_table.comment if raw_table.comment else _PLACEHOLDER_COMMENT,
        filters=_get_placeholder_filter(),
        dimensions=dimensions,
        time_dimensions=time_dimensions,
        measures=measures,
    )


def raw_schema_to_semantic_context(
    base_tables: List[str], #send fully qualified table names
    semantic_model_name: str,
    conn,
    n_sample_values: int = _DEFAULT_N_SAMPLE_VALUES_PER_COL,
    allow_joins: Optional[bool] = False,
) :
    """
    Converts a list of fully qualified Snowflake table names into a semantic model.

    Parameters:
    - base_tables  (list[str]): Fully qualified table names to include in the semantic model.
    - snowflake_account (str): Snowflake account identifier.
    - semantic_model_name (str): A meaningful semantic model name.
    - conn (SnowflakeConnection): SnowflakeConnection to reuse.
    - n_sample_values (int): The number of sample values per col.

    Returns:
    - The semantic model (semantic_model_pb2.SemanticModel).

    This function fetches metadata for the specified tables, performs schema validation, extracts key information,
    enriches metadata from the Snowflake database, and constructs a semantic model in protobuf format.
    It handles different databases and schemas within the same account by creating unique Snowflake connections as needed.

    Raises:
    - AssertionError: If no valid tables are found in the specified schema.
    """

    # For FQN tables, create a new snowflake connection per table in case the db/schema is different.
    table_objects = []
    unique_database_schema: List[str] = []
    
    for table in base_tables:
     
        split=table.split(".")
        db_name=split[0]
        schema=split[1]
        table_name=split[2]

       

        logger.info(f"Pulling column information from {table_name}")
        valid_schemas_tables_columns_df = get_valid_schemas_tables_columns_df(
            conn=conn,
            db_name=db_name,
            table_schema=schema,
            table_names=[table_name],
        )


        assert not valid_schemas_tables_columns_df.empty

        # get the valid columns for this table.
        valid_columns_df_this_table = valid_schemas_tables_columns_df[
            valid_schemas_tables_columns_df["TABLE_NAME"] == table_name
        ]


        raw_table = get_table_representation(
            conn=conn,
            schema_name=schema,  # Fully-qualified schema
            table_name=table_name,  # Non-qualified table name
            table_index=0,
            ndv_per_column=n_sample_values,  # number of sample values to pull per column.
            columns_df=valid_columns_df_this_table,
            max_workers=1,
        )


        table_object = _raw_table_to_semantic_context_table(
            database=db_name,
            schema=schema,
            raw_table=raw_table,
        )
        table_objects.append(table_object)
    # TODO(jhilgart): Call cortex model to generate a semantically friendly name here.

    placeholder_relationships = _get_placeholder_joins() if allow_joins else None
    context = semantic_model_pb2.SemanticModel(
        name=semantic_model_name,
        tables=table_objects,
        relationships=placeholder_relationships,
    )

    return context


def comment_out_section(yaml_str: str, section_name: str) -> str:
    """
    Comments out all lines in the specified section of a YAML string.

    Parameters:
    - yaml_str (str): The YAML string to process.
    - section_name (str): The name of the section to comment out.

    Returns:
    - str: The modified YAML string with the specified section commented out.
    """
    updated_yaml = []
    lines = yaml_str.split("\n")
    in_section = False
    section_indent_level = 0

    for line in lines:
        stripped_line = line.strip()

        # When we find a section with the provided name, we can start commenting out lines.
        if stripped_line.startswith(f"{section_name}:"):
            in_section = True
            section_indent_level = len(line) - len(line.lstrip())
            comment_indent = " " * section_indent_level
            updated_yaml.append(f"{comment_indent}# {line.strip()}")
            continue

        # Since this method parses a raw YAML string, we track whether we're in the section by the indentation level.
        # This is a pretty rough heuristic.
        current_indent_level = len(line) - len(line.lstrip())
        if (
            in_section
            and current_indent_level <= section_indent_level
            and stripped_line
        ):
            in_section = False

        # Comment out the field and its subsections, preserving the indentation level.
        if in_section and line.strip():
            comment_indent = " " * current_indent_level
            updated_yaml.append(f"{comment_indent}# {line.strip()}")
        else:
            updated_yaml.append(line)

    return "\n".join(updated_yaml)


def append_comment_to_placeholders(yaml_str: str) -> str:
    """
    Finds all instances of a specified placeholder in a YAML string and appends a given text to these placeholders.
    This is the homework to fill out after your yaml is generated.

    Parameters:
    - yaml_str (str): The YAML string to process.

    Returns:
    - str: The modified YAML string with appended text to placeholders.
    """
    updated_yaml = []
    # Split the string into lines to process each line individually
    lines = yaml_str.split("\n")

    for line in lines:
        # Check if the placeholder is in the current line.
        # Strip the last quote to match.
        if line.rstrip("'").endswith(_PLACEHOLDER_COMMENT):
            # Replace the _PLACEHOLDER_COMMENT with itself plus the append_text
            updated_line = line + _FILL_OUT_TOKEN
            updated_yaml.append(updated_line)
        elif line.rstrip("'").endswith(AUTOGEN_TOKEN):
            updated_line = line + _AUTOGEN_COMMENT_TOKEN
            updated_yaml.append(updated_line)
        # Add comments to specific fields in certain sections.
        elif line.lstrip().startswith("join_type"):
            updated_line = line + _FILL_OUT_TOKEN + "  supported: inner, left_outer"
            updated_yaml.append(updated_line)
        elif line.lstrip().startswith("relationship_type"):
            updated_line = (
                line + _FILL_OUT_TOKEN + " supported: many_to_one, one_to_one"
            )
            updated_yaml.append(updated_line)
        else:
            updated_yaml.append(line)

    # Join the lines back together into a single string
    return "\n".join(updated_yaml)


def _to_snake_case(s: str) -> str:
    """
    Convert a string into snake case.

    Parameters:
    s (str): The string to convert.

    Returns:
    str: The snake case version of the string.
    """
    # Replace common delimiters with spaces
    s = s.replace("-", " ").replace("_", " ")

    words = s.split(" ")

    # Convert each word to lowercase and join with underscores
    snake_case_str = "_".join([word.lower() for word in words if word]).strip()

    return snake_case_str


def generate_base_semantic_model_from_snowflake(
    base_tables: List[str],
    conn,
    semantic_model_name: str,
    n_sample_values: int = _DEFAULT_N_SAMPLE_VALUES_PER_COL,
    output_yaml_path: Optional[str] = None,
) -> None:
    """
    Generates a base semantic context from specified Snowflake tables and exports it to a YAML file.

    Parameters:
        base_tables : Fully qualified names of Snowflake tables to include in the semantic context.
        conn: SnowflakeConnection to reuse.
        snowflake_account: Identifier of the Snowflake account.
        semantic_model_name: The human readable model name. This should be semantically meaningful to an organization.
        output_yaml_path: Path for the output YAML file. If None, defaults to 'semantic_model_generator/output_models/YYYYMMDDHHMMSS_<semantic_model_name>.yaml'.
        n_sample_values: The number of sample values to populate for all columns.

    Returns:
        None. Writes the semantic context to a YAML file.
    """
    formatted_datetime = datetime.now().strftime("%Y%m%d%H%M%S")
    if not output_yaml_path:
        file_name = f"{formatted_datetime}_{_to_snake_case(semantic_model_name)}.yaml"
        if os.path.exists("semantic_model_generator/output_models"):
            write_path = f"semantic_model_generator/output_models/{file_name}"
        else:
            write_path = f"./{file_name}"
    else:  # Assume user gives correct path.
        write_path = output_yaml_path

    yaml_str = generate_model_str_from_snowflake(
        base_tables,
        n_sample_values=n_sample_values if n_sample_values > 0 else 1,
        semantic_model_name=semantic_model_name,
        conn=conn,
    )

    with open(write_path, "w") as f:
        # Clarify that the YAML was autogenerated and that placeholders should be filled out/deleted.
        f.write(_AUTOGEN_COMMENT_WARNING)
        f.write(yaml_str)

    logger.info(f"Semantic model saved to {write_path}")

    return None


def generate_model_str_from_snowflake(
    base_tables: List[str],
    semantic_model_name: str,
    conn,
    n_sample_values: int = _DEFAULT_N_SAMPLE_VALUES_PER_COL,
    allow_joins: Optional[bool] = False,
) -> str:
    """
    Generates a base semantic context from specified Snowflake tables and returns the raw string.

    Parameters:
        base_tables : Fully qualified names of Snowflake tables to include in the semantic context.
        semantic_model_name: The human readable model name. This should be semantically meaningful to an organization.
        conn: SnowflakeConnection to reuse.
        n_sample_values: The number of sample values to populate for all columns.
        allow_joins: Whether to allow joins in the semantic context.

    Returns:
        str: The raw string of the semantic context.
    """
   
    context = raw_schema_to_semantic_context(
        base_tables,
        n_sample_values=n_sample_values if n_sample_values > 0 else 1,
        semantic_model_name=semantic_model_name,
        allow_joins=allow_joins,
        conn=conn,
    )
    # Validate the generated yaml is within context limits.
    # We just throw a warning here to allow users to update.
    validate_context_length(context)

    yaml_str = proto_to_yaml(context)
    # Once we have the yaml, update to include to # <FILL-OUT> tokens.
    yaml_str = append_comment_to_placeholders(yaml_str)
    # Comment out the filters section as we don't have a way to auto-generate these yet.
    yaml_str = comment_out_section(yaml_str, "filters")
    yaml_str = comment_out_section(yaml_str, "relationships")

    return yaml_str
